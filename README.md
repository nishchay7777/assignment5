Database Pipeline Project
Hey there! Welcome to my Database Pipeline Project! This little tool is something I whipped up in Python to make handling data from a MySQL database a breeze. It’s all about pulling data out and saving it in handy formats like CSV, Parquet, and Avro, plus it can automate the whole process. I built it with a scheduling feature that kicks off every hour, and there’s even a basic check for database changes to keep things running smoothly. It’s been a fun challenge to put together, and I hope it saves you some time too!
If you’re ready to give it a spin, you’ll need to set it up with your own MySQL details first. Just tweak the connection settings in the code—things like the host, username (set to "root" for now), password ("nishchay@54321"), and the database name ("sample_db"). Make sure you’ve got MySQL up and running with at least one table, like "employees," to test it out. You’ll also need to install a few packages—pandas, mysql-connector-python, pyarrow, fastavro, schedule, and sqlalchemy—with a quick pip install command. Once that’s done, just run the script, and watch it work its magic!
One of the coolest parts is the replication features. You can copy the entire database to another one or pick specific tables and columns to move over—perfect for when you need to be picky about data. To try it out, uncomment the replication calls at the bottom of the script and plug in your destination database name. I’ve tested it as best I could, but give it a quick run on your setup to make sure everything lines up. Feel free to tweak the schedule or add more triggers if you need to—consider it a starting point to make your own! Good luck, and let me know how it goes!
